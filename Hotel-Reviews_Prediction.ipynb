{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hotel Review Sentiment Analysis Part 3: Prediction\n",
    "## Adura ABIONA, PhD (UNSW)\n",
    "### 4 May, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This is the **Part 3** of the **Hotel Review Sentiment Analysis** of Australian hotels, from four major cities (Canberra, Sydney, Melbourne and Brisbane), based on reviewers' opionions (on a numerical scale of 1-5) from [**TripAdvisor**](http://www.tripadvisor.com.au) website. This part is focused on **Prediction** of the reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20.1\n"
     ]
    }
   ],
   "source": [
    "import glob, os, string # os.chdir()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt #import matplotlib as mpl\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, roc_auc_score\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import bigrams, trigrams\n",
    "sep = \"~\"\n",
    "DataDir = \"Datasets/\"\n",
    "#nltk.download() # download the english stopwords corpus and the punkt package and maybe the porter stemmer if not present\n",
    "\n",
    "pd.set_option('display.max_columns', 36)\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The block of code below reads the review details for the hotels from the 4 major cities in Australia into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27868, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>rati</th>\n",
       "      <th>value</th>\n",
       "      <th>locat</th>\n",
       "      <th>sleep</th>\n",
       "      <th>rooms</th>\n",
       "      <th>clean</th>\n",
       "      <th>servi</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>review_478470647</td>\n",
       "      <td>“Waste of money”</td>\n",
       "      <td>From the moment we walked into the Adobe,we kn...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>review_476951438</td>\n",
       "      <td>“Well appointed room”</td>\n",
       "      <td>On check in was a queue of 6 waiting with only...</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>review_476646112</td>\n",
       "      <td>“Forgotten property \"vanished into thin air\" a...</td>\n",
       "      <td>Review submitted on behalf of my wife who was ...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>review_475716850</td>\n",
       "      <td>“Everything you need and more”</td>\n",
       "      <td>This is a great hotel. Clean and really comfor...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>review_474490948</td>\n",
       "      <td>“Super handy for shops, food and transport”</td>\n",
       "      <td>This is my go to accommodation in Canberra. Cl...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                              title  \\\n",
       "0  review_478470647                                   “Waste of money”   \n",
       "1  review_476951438                              “Well appointed room”   \n",
       "2  review_476646112  “Forgotten property \"vanished into thin air\" a...   \n",
       "3  review_475716850                     “Everything you need and more”   \n",
       "4  review_474490948        “Super handy for shops, food and transport”   \n",
       "\n",
       "                                                body rati value  locat  sleep  \\\n",
       "0  From the moment we walked into the Adobe,we kn...    1     1    NaN    NaN   \n",
       "1  On check in was a queue of 6 waiting with only...    3   NaN    NaN    NaN   \n",
       "2  Review submitted on behalf of my wife who was ...    2     4    NaN    NaN   \n",
       "3  This is a great hotel. Clean and really comfor...    5   NaN    NaN    NaN   \n",
       "4  This is my go to accommodation in Canberra. Cl...    5   NaN    NaN    NaN   \n",
       "\n",
       "   rooms  clean  servi  other  \n",
       "0    2.0    NaN    1.0    NaN  \n",
       "1    NaN    NaN    NaN    NaN  \n",
       "2    4.0    NaN    2.0    NaN  \n",
       "3    NaN    NaN    NaN    NaN  \n",
       "4    NaN    NaN    NaN    NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_feats = ['id', 'title','body','rati','value','locat','sleep','rooms','clean','servi','other']\n",
    "citys = ['Canberra', 'Sydney', 'Melbourne', 'Brisbane']\n",
    "review_df = pd.DataFrame() #creates a new dataframe that's empty\n",
    "for city in citys:\n",
    "    citydir = os.path.join(os.getcwd(), DataDir + city)\n",
    "    for file in glob.glob(os.path.join(citydir,\"*-review.mcsv\")): \n",
    "        review_df = review_df.append(pd.read_csv(os.path.join(citydir, file), sep=sep, header=None, names = review_feats), ignore_index=True)\n",
    "\n",
    "print(review_df.shape)\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Having a looking at some of the reviews before being processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the moment we walked into the Adobe,we knew it was a mistake. Two young women behind the counter looked at us like we were scum and never broken a smile. The rooms are small and out dated for the money paid. No secure parking. You park in a general car park next to a bus depot. We barely slept, one because the beds are hard and because we were concerned for our new car being parked out for all to get to.My husband and I were in town from Newcastle to see a very sick friend for the weekend. Staying here just made the whole weekend even more stressful. Just horrible. We will not be back.\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "On check in was a queue of 6 waiting with only one person on reception- room well appointed- mine had no view - bed comfortable- shower wasn't great pressure - breakfast was great hollandaise sauce was perfect \n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Review submitted on behalf of my wife who was the staying guest on 29 March 2017: The room was comfortable and it was a pleasant stay. Unfortunately this was entirely soured when I left my Seagate 2 Tb portable USB hard drive behind upon checking out. I had been using the hard drive the previous day and its USB cable was still attached. Whilst packing I set it aside on the bed as I have a purpose-bought carry-case for it and I intended to pack it into the case before putting it in my hand luggage. Upon arriving home I found the empty carry-case in my hand luggage and immediately realised that I had left the portable hard drive with the attached USB cable on the bed in the room. I did not arrive home until late evening, so I phoned the Abode Hotel first thing next morning. They advised that they would search and get back to me later. When I had not been contacted several hours later I phoned again and they informed me that no portable hard drive was not found in the room. I am very upset about this as I have no doubt it was left there.  To make matters worse I recently disposed of a computer so the portable hard drive has 17 years of family photos on it (including the entire life of my 17 year old son), and it also has ALL of my current semester’s work on my Masters study at university. I can only conclude that either it has been kept by a hotel staff member or that the room was so poorly serviced after my departure that it was overlooked and subsequently taken by the next guest. This has been extremely distressing.\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "This is a great hotel. Clean and really comfortable room. The kitchenette is very well stocked with everything you need. Good size for families. The Pantry downstairs in the foyer is great. Good selection and not expensive for the convenience. Don't worry about the distance from the CBD. Everything is close on the roads in Canberra. We'll be back\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "This is my go to accommodation in Canberra. Clean, affordable and close to bus station, shops and restaurants. Only 20 min bus ride from the city. Free wifi. Visit the Hellenic Club 3 min walk away - Chinese restaurant is amazing. Downstairs at the Abode they have a great restaurant. Wings are a winner.\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_dfx = review_df[['rati', 'body']]\n",
    "for idx in range(5):\n",
    "    print(review_dfx.body[idx])\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the reviews by removing stopwords, punctuations, stemming and tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Some words (e.g. no, not, more, most etc.) have been removed from the standard stopwords available in NLTK. \n",
    "#It’s done so because those words can have some sentiment impact in our review dataset.\n",
    "#nltk.download() # download the english stopwords corpus and the punkt package and maybe the porter stemmer if not present\n",
    "\n",
    "custom_stopwords = set(stopwords.words('english') + [\"n't\", \"'ve\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS) \n",
    "                 + ['canberra', 'sydney', 'melbourne', 'brisbane']) - set(('over', 'under', 'below', 'more', \n",
    "                    'most', 'no', 'not', 'only', 'such', 'few', 'so', 'too', 'very', 'just', 'any', 'once'))\n",
    "\n",
    "\n",
    "def restring_tokens(token_list):\n",
    "    return ' '.join(token_list)\n",
    "\n",
    "def preprocess(dfSent):\n",
    "    sentx = str(dfSent)\n",
    "    for dg in string.digits:  sentx = sentx.replace(dg, \" \") \n",
    "    for ch in string.punctuation:  sentx = sentx.replace(ch, \" \") \n",
    "    sentx = sentx.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    sentx = sentx.lower()    \n",
    "    wordList = [word for word in sentx.split() if word not in custom_stopwords] # Given a list of words, remove any that are in a list of stop words.\n",
    "    sentx = ' '.join(wordList)\n",
    "    token_list = nltk.word_tokenize(sentx) \n",
    "    STEMMER = PorterStemmer()\n",
    "    token_list = [STEMMER.stem(tok) for tok in token_list]\n",
    "    return restring_tokens(token_list)\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    token_list = nltk.word_tokenize(s)\n",
    "    exclude_stopwords = lambda token : token not in NLTK_STOPWORDS\n",
    "    return ' '.join(filter(exclude_stopwords, token_list))\n",
    "\n",
    "def filter_out_more_stopwords(token_list):\n",
    "    return filter(lambda tok : tok not in MORE_STOPWORDS, token_list)\n",
    "\n",
    "def processTokens(s):\n",
    "    s = s.translate(None, string.digits)\n",
    "    s = s.lower()\n",
    "    s = s.translate(None, string.punctuation)\n",
    "    s = remove_stopwords(s)\n",
    "    token_list = nltk.word_tokenize(s)\n",
    "    token_list = filter_out_more_stopwords(token_list)\n",
    "    STEMMER = PorterStemmer()\n",
    "    token_list = [STEMMER.stem(tok.decode('utf-8')) for tok in token_list]\n",
    "    return restring_tokens(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Having a looking at some of the reviews after being processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 32s, sys: 938 ms, total: 3min 33s\n",
      "Wall time: 3min 36s\n",
      "\n",
      "moment walk adob knew mistak young women counter look like scum broken smile room small date money paid no secur park park gener car park bu depot bare slept bed hard concern new car park husband town newcastl veri sick friend weekend stay just weekend more stress just horribl not\n",
      "\n",
      "check queue wait onli person recept room appoint no view bed comfort shower great pressur breakfast great hollandais sauc perfect\n",
      "\n",
      "review submit behalf wife stay guest march room comfort pleasant stay unfortun entir sour left seagat tb portabl usb hard drive check use hard drive previou day usb cabl attach whilst pack set asid bed purpos bought carri case intend pack case put hand luggag arriv home carri case hand luggag immedi realis left portabl hard drive attach usb cabl bed room not arriv home late even so phone abod hotel thing morn advis search later not contact hour later phone inform no portabl hard drive not room veri upset no doubt left make matter wors recent dispos comput so portabl hard drive year famili photo includ entir life year old son current semest ’ s work master studi univers onli conclud kept hotel staff member room so poorli servic departur overlook subsequ taken guest extrem distress\n",
      "\n",
      "great hotel clean realli comfort room kitchenett veri stock need good size famili pantri downstair foyer great good select not expens conveni worri distanc cbd close road\n",
      "\n",
      "accommod clean afford close bu station shop restaur onli min bu ride citi free wifi visit hellen club min walk away chines restaur amaz downstair abod great restaur wing winner\n"
     ]
    }
   ],
   "source": [
    "review_dfy = review_df\n",
    "%time review_dfy['body'] = review_dfy['body'].apply(preprocess)\n",
    "\n",
    "for idx in range(5):\n",
    "    print()\n",
    "    print(review_dfy.body[idx])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 4s, sys: 2.92 s, total: 3min 7s\n",
      "Wall time: 3min 8s\n",
      "\n",
      "moment walk adob knew mistak young women counter look like scum broken smile room small date money paid no secur park park gener car park bu depot bare slept bed hard concern new car park husband town newcastl veri sick friend weekend stay just weekend more stress just horribl not\n",
      "\n",
      "check queue wait onli person recept room appoint no view bed comfort shower great pressur breakfast great hollandai sauc perfect\n",
      "\n",
      "review submit behalf wife stay guest march room comfort pleasant stay unfortun entir sour left seagat tb portabl usb hard drive check use hard drive previou day usb cabl attach whilst pack set asid bed purpo bought carri case intend pack case hand luggag arriv home carri case hand luggag immedi reali left portabl hard drive attach usb cabl bed room not arriv home late so phone abod hotel thing morn advi search later not contact hour later phone inform no portabl hard drive not room veri upset no doubt left make matter wor recent dispo comput so portabl hard drive year famili photo includ entir life year old son current semest ’ work master studi univ onli conclud kept hotel staff member room so poorli servic departur overlook subsequ taken guest extrem distress\n",
      "\n",
      "great hotel clean realli comfort room kitchenett veri stock need good size famili pantri downstair foyer great good select not expen conveni worri distanc cbd close road\n",
      "\n",
      "accommod clean afford close bu station shop restaur onli min bu ride citi free wifi visit hellen club min walk away chine restaur amaz downstair abod great restaur wing winner\n"
     ]
    }
   ],
   "source": [
    "review_dfz = review_df\n",
    "%time review_dfz['body'] = review_dfz['body'].apply(preprocess)\n",
    "\n",
    "for idx in range(5):\n",
    "    print()\n",
    "    print(review_dfz.body[idx])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(review_dfz.body, review_dfz.rati, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniVect = CountVectorizer(analyzer = \"word\",  tokenizer = None, preprocessor = None, \n",
    "                         ngram_range = (1, 1), binary = False, strip_accents='unicode')\n",
    "\n",
    "biVect = CountVectorizer(analyzer = \"word\",  tokenizer = None, preprocessor = None, ngram_range = (2, 2), strip_accents='unicode')\n",
    "\n",
    "triVect = CountVectorizer(analyzer = \"word\",  tokenizer = None, preprocessor = None, ngram_range = (3, 3), strip_accents='unicode')\n",
    "\n",
    "\n",
    "bi_triVect = CountVectorizer(analyzer = \"word\",  tokenizer = None, preprocessor = None, ngram_range = (2, 3), strip_accents='unicode')\n",
    "\n",
    "rfVect = CountVectorizer(analyzer = \"word\",  tokenizer = None, preprocessor = None, ngram_range = (2, 2), \n",
    "                         strip_accents='unicode', max_features = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization through Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_confusion_matrix_relative(confusion_matrix):\n",
    "    star_category_classes = [1, 2, 3, 4, 5]\n",
    "    N = list(map(lambda clazz : sum(Y_test == clazz), star_category_classes))\n",
    "    relative_confusion_matrix = np.empty((len(star_category_classes), len(star_category_classes)))\n",
    "    \n",
    "    for j in range(0, len(star_category_classes)):\n",
    "        if N[j] > 0:\n",
    "            relative_frequency = confusion_matrix[j, :] / float(N[j])\n",
    "            relative_confusion_matrix[j, :] = relative_frequency\n",
    "            \n",
    "    return relative_confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix=[[]], title='CM', savefilename=''):\n",
    "    rcm = make_confusion_matrix_relative(confusion_matrix)\n",
    "    #plt.imshow(rcm, vmin=0, vmax=1, interpolation='nearest')\n",
    "    c = plt.pcolor(rcm, edgecolors='k', linewidths=4, cmap='jet', vmin=0.0, vmax=1.0)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(0.5 + np.arange(5), np.arange(1,6))\n",
    "    plt.yticks(0.5 + np.arange(5), np.arange(1,6))\n",
    "\n",
    "    def show_values(pc, fmt=\"%.2f\", **kw):\n",
    "        #from itertools import zip\n",
    "        pc.update_scalarmappable()\n",
    "        ax = pc.get_axes()\n",
    "        for p, color, value in zip(pc.get_paths(), pc.get_facecolors(), pc.get_array()):\n",
    "            x, y = p.vertices[:-2, :].mean(0)\n",
    "            if sum(color[:2] > 0.3) >= 2:\n",
    "                color = (0.0, 0.0, 0.0)\n",
    "            else:\n",
    "                color = (1.0, 1.0, 1.0)\n",
    "            ax.text(x, y, fmt % value, ha=\"center\", va=\"center\", color=color, **kw)\n",
    "    \n",
    "    show_values(c)\n",
    "    if savefilename:\n",
    "        plt.savefig(savefilename, bbox_inches='tight')\n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "def print_classifier_performance_metrics(name, predictions):\n",
    "    target_names = ['1 star', '2 star', '3 star', '4 star', '5 star']\n",
    "    \n",
    "    print (\"MODEL: %s\" % name)\n",
    "    print ()\n",
    "\n",
    "    print ('Precision: ' + str(metrics.precision_score(Y_test, predictions, average='micro')))\n",
    "    print ('Recall: ' + str(metrics.recall_score(Y_test, predictions, average='micro')))\n",
    "    print ('F1: ' + str(metrics.f1_score(Y_test, predictions,  average='micro')))\n",
    "    print ('Accuracy: ' + str(metrics.accuracy_score(Y_test, predictions)))\n",
    "\n",
    "    print()\n",
    "    print ('Classification Report:')\n",
    "    print (classification_report(Y_test, predictions, target_names=target_names))\n",
    "    \n",
    "    print()\n",
    "    print ('Precision variance: %f' % np.var(precision_score(Y_test, predictions, average=None), ddof=len(target_names)-1))\n",
    "    \n",
    "    print()\n",
    "    print ('Recall variance: %f' % np.var(recall_score(Y_test, predictions, average=None), ddof=len(target_names)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Model\n",
    "#### Transforming the hotel reviews into feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: (array([2.0, 4, '5', ..., 5, 4, 5], dtype=object),)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-66825c0ddaa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Make predictions with Unigram Multinomial NB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0muniNb_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0muniNb_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0muniNb_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muniNb_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0mlabelbin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabelbin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabelbin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_input_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0m_unique_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FN_UNIQUE_LABELS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_unique_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mys_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: (array([2.0, 4, '5', ..., 5, 4, 5], dtype=object),)"
     ]
    }
   ],
   "source": [
    "uniTrain = uniVect.fit_transform(X_train)\n",
    "uniTest = uniVect.transform(X_test)\n",
    "uniTrain, uniTest\n",
    "\n",
    "#Make predictions with Unigram Multinomial NB\n",
    "uniNb_classifier = MultinomialNB()\n",
    "uniNb_classifier.fit(uniTrain, Y_train)\n",
    "uniNb_prediction = uniNb_classifier.predict(uniTest)\n",
    "\n",
    "uniNbConfMatrix = confusion_matrix(Y_test, uniNb_prediction)\n",
    "print (make_confusion_matrix_relative(uniNbConfMatrix))\n",
    "plot_confusion_matrix(uniNbConfMatrix, 'Multinomial Naive Bayes Confusion Matrix', savefilename='MultinomialCM.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniTrain\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_classifier_performance_metrics('Multinomial Naive Bayes', uniNb_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bigram Naive Bayes Model\n",
    "#### Transform hotel reviews into feature vectorizers by counting bigram occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biTrain = biVect.fit_transform(X_train)\n",
    "biTest = biVect.transform(X_test)\n",
    "biTrain, biTest\n",
    "\n",
    "#Make predictions with Bigram Multinomial NB\n",
    "bigram_multinomial_nb_classifier = MultinomialNB().fit(bigram_multinomial_feature_matrix_train, Y_train)\n",
    "bigram_multinomial_nb_prediction = bigram_multinomial_nb_classifier.predict(bigram_multinomial_feature_matrix_test)\n",
    "\n",
    "#Visualize through confusion matrix\n",
    "bigram_multinomial_confusion_matrix = confusion_matrix(Y_test, bigram_multinomial_nb_prediction)\n",
    "print (make_confusion_matrix_relative(multinomial_confusion_matrix))\n",
    "plot_confusion_matrix(bigram_multinomial_confusion_matrix, 'Bigram Multinomial Naive Bayes Confusion Matrix', savefilename='BigramMultinomialCM.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_classifier_performance_metrics('Bigram Multinomial Naive Bayes', bigram_multinomial_nb_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Naive Bayes Model\n",
    "\n",
    "#### Transform hotel reviews into feature vectorizers by counting trigram occurrences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_multinomial_feature_matrix_train = trigram_vectorizer.fit_transform(X_train)\n",
    "trigram_multinomial_feature_matrix_test = trigram_vectorizer.transform(X_test)\n",
    "#trigram_multinomial_feature_matrix_train, trigram_multinomial_feature_matrix_test\n",
    "\n",
    "#Make predictions with Trigram Multinomial NB\n",
    "tri_gram_multinomial_nb_classifier = MultinomialNB().fit(trigram_multinomial_feature_matrix_train, Y_train)\n",
    "tri_gram_multinomial_nb_prediction = tri_gram_multinomial_nb_classifier.predict(trigram_multinomial_feature_matrix_test)\n",
    "\n",
    "#Visualize through confusion matrix\n",
    "trigram_multinomial_confusion_matrix = confusion_matrix(Y_test, tri_gram_multinomial_nb_prediction)\n",
    "plot_confusion_matrix(trigram_multinomial_confusion_matrix, 'Trigram Multinomial Naive Bayes Confusion Matrix', savefilename='TrigramMultinomialCM.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_classifier_performance_metrics('Trigram Multinomial Naive Bayes', tri_gram_multinomial_nb_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest 100 Learners Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest100 = RandomForestClassifier(n_estimators = 100, random_state=42)\n",
    "\n",
    "#Transform Yelp reviews into feature vectors\n",
    "random_forest_feature_matrix_train = random_forest_vectorizer.fit_transform(X_train)\n",
    "random_forest_feature_matrix_test = random_forest_vectorizer.transform(X_test)\n",
    "\n",
    "#Make predictions with random forest set at 100 learners\n",
    "%time forest100.fit(random_forest_feature_matrix_train.toarray(), Y_train)\n",
    "forest100_pred = forest100.predict(random_forest_feature_matrix_test.toarray())\n",
    "np.save('forest100pred', forest100_pred)\n",
    "\n",
    "#Visualize results in confusion matrix\n",
    "random_forest_confusion_matrix = confusion_matrix(Y_test, forest100_pred)\n",
    "plot_confusion_matrix(random_forest_confusion_matrix, 'Random Forest (100 Learners) Confusion Matrix', savefilename='RandomForestCM.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_classifier_performance_metrics('Random Forest (100 Learners)', forest100_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
