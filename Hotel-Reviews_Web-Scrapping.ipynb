{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hotel Review Sentiment Analysis Part 1: Web Scrapping\n",
    "## Adura ABIONA, PhD (UNSW)\n",
    "### 4 May, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This is an attempt to use sentimental analysis to analyse Australian hotels, from four major cities (Canberra, Sydney, Melbourne and Brisbane), based on reviewers' opionions (on a numerical scale of 1-5) from [**TripAdvisor**](http://www.tripadvisor.com.au) website.  \n",
    "\n",
    "**Part I** of this work is based on data acquisition through web scraping from TripAdvisor using [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/).\n",
    "\n",
    "[**TripAdvisor**](http://www.tripadvisor.com.au) is an online hotel review organisation. It allows hotel customers in many nations to write comments about their experiences in the hotel they stayed and rate the hotel's servion on a numerical scale of 1-5. \n",
    "\n",
    "The web scraping from tripAdvisor website starts with the ***hotel_run(base_url, sub_url, city, cutNo_hotel, cutNo_review)*** and Writes the review details of each hotel into hotelName-review.mcsv file in the city folder.\n",
    "This function calls the following functions:    \n",
    "\n",
    "**1.** ***hotel_Urls(base_url, sub_url, city)*** function uses BeautifulSou to scrap urls of all the hotel review lists from the *base_url + sub_url* page. These hotel urls are on more than two pages. It then calls the hotel_details() for each hotel review list with its page url as input.\n",
    "        \n",
    "**2.** ***hotel_details(soup_Page, city)*** function uses BeautifulSoup to scrap hotel review details from each page url of the hotel review list. The deatils for all the reviews are saved as city2_hotel_file.mcsv file.\n",
    "        \n",
    "**3.** ***hotel_review(page_url)*** function finds all reviews on a page and extract url for each review and process it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup #To scrape the data from tripAdvisor website\n",
    "import os, shutil\n",
    "from datetime import datetime\n",
    "sep = \"~\"\n",
    "dtfmt = '%Y-%m-%d %H:%M:%S'\n",
    "DataDir = \"Datasets/\" \n",
    "\n",
    "def hotel_Urls(base_urlx, sub_urlx, cityx): \n",
    "    # This function uses BeautifulSou to scrape urls of all the hotel review lists from the \"base_url + sub_url\" page. \n",
    "    #These urls are on more than two pages.\n",
    "    # It then calls the hotel_details() for each hotel review list with its page url as input.\n",
    "        \n",
    "    soup_Page = BeautifulSoup(urlopen(base_urlx + sub_urlx), 'lxml')\n",
    "    div = soup_Page.find('div', {'class': 'unified pagination standard_pagination'})\n",
    "    if div.find(\"span\", {'class': 'nav next ui_button disabled'}) != None: # Last page\n",
    "        hotel_details(soup_Page, cityx)\n",
    "    elif div.find('span', {'class': 'nav previous ui_button disabled'})!=None: # First page\n",
    "        hotel_details(soup_Page, cityx)\n",
    "        urlmen = div.find('a', href = True)\n",
    "        urlmen = urlmen['href']\n",
    "        hotel_Urls(base_urlx, urlmen, cityx)\n",
    "    else:\n",
    "        hotel_details(soup_Page, cityx)\n",
    "        urlmen = div.find_all('a', href = True)\n",
    "        urlmen = urlmen[1]['href']\n",
    "        hotel_Urls(base_urlx, urlmen, cityx)\n",
    "\n",
    "def hotel_details(soup_Page, city2):\n",
    "    # This function uses BeautifulSoup to scrape hotel review details from each page url of the hotel review list.\n",
    "    # The details are: rating, number of reviews and ecah review page url of the hotel. \n",
    "    # The deatils for all the review lists for all the hotels are then saved in the  city_Folder as city2_hotel_file.mcsv file.\n",
    "        \n",
    "    hotel_info = \"\"\n",
    "    hotel_Page = soup_Page.find_all(\"div\", {\"class\": \"listing_rating\"}) \n",
    "    \n",
    "    for hotel_details in hotel_Page:  \n",
    "        hotel_rating = hotel_details.find(\"span\")\n",
    "        if hotel_rating != None: hotel_rating = hotel_rating['alt'][0]\n",
    "        else: hotel_rating = \"nan\"\n",
    "        hotel_url = hotel_details.find(\"a\")\n",
    "        if hotel_url != None:\n",
    "            Review_No = hotel_url.find(text=True)\n",
    "            Review_No = Review_No.split(\" \")[0]\n",
    "            hotel_url = hotel_url['href']     \n",
    "            hotel_url_parts = hotel_url.split(\"-\")  \n",
    "            head_url = hotel_url_parts[0] + \"-\" + hotel_url_parts[1] + \"-\" +  hotel_url_parts[2] + \"-\" + hotel_url_parts[3] + \"-\" \n",
    "            tail_url = hotel_url_parts[4] + \"-\" + hotel_url_parts[5]           \n",
    "            #id[2] + sep + name[4]     \n",
    "            hotel_detail = hotel_url_parts[2] + sep + hotel_url_parts[4] + sep + head_url + sep + tail_url + sep + hotel_rating + sep + Review_No\n",
    "            hotel_info = hotel_info + hotel_detail + \"\\n\"\n",
    "    hotel_file = open(DataDir + city2 + \"/\" + city2 + \"_hotel_file.mcsv\", \"a+\")\n",
    "    hotel_file.write(hotel_info)\n",
    "    hotel_file.close()\n",
    "\n",
    "def hotel_review(page_url):\n",
    "    # This function finds all reviews on a page and extract url for each review process it. \n",
    "    # It gets the review title, body, overall rating (on the scale 0f 1-5) and other attribute ratings and these details\n",
    "    \n",
    "    page_review_string = \"\"\n",
    "    page_review = \"\"\n",
    "    soup = BeautifulSoup(urlopen(page_url), \"lxml\")\n",
    "    \n",
    "    #find all reviews on page and extract urls\n",
    "    reviews_onpage = soup.find_all(\"div\", {\"class\": \"reviewSelector\"})\n",
    "    review_urls = []\n",
    "    #extract urls from each review and get a list of review urls\n",
    "    for review in reviews_onpage:\n",
    "        review_url = review.find('a', href = True)\n",
    "        review_id = review['id']\n",
    "        if str(type(review_url)) != \"<type 'NoneType'>\":\n",
    "            review_url = review_url['href']\n",
    "            curr_review = {'id' : review_id, 'url' : review_url}\n",
    "            review_urls.append(curr_review)\n",
    "     \n",
    "    #begin processing reviews\n",
    "    for url in review_urls:\n",
    "        #construct url and request html\n",
    "        soup = BeautifulSoup(urlopen(base_url + url['url']), \"lxml\")\n",
    "        \n",
    "        highlight_review = soup.find(\"div\", {\"id\" : url['id']})        \n",
    "        if highlight_review is not None:\n",
    "            title = body = rati = value = locat = sleep = rooms = clean = servi = other = \"nan\"\n",
    "            \n",
    "            review_title  = highlight_review.find(\"div\", {\"property\": \"name\"})  \n",
    "            if review_title != None: title = review_title.getText()\n",
    "                \n",
    "            review_rating = highlight_review.find(\"div\", {\"class\": \"rating reviewItemInline\"})\n",
    "            if review_rating != None: \n",
    "                review_rating = review_rating.find(\"img\")  \n",
    "                if review_rating != None: rati = review_rating['alt'][0]\n",
    "                    \n",
    "            review_body = highlight_review.find(\"p\", {\"property\": \"reviewBody\"})\n",
    "            if review_body != None: \n",
    "                body =  review_body.getText(separator=' ') \n",
    "                body = body.replace('\\n', '').replace('\\r', '')\n",
    "\n",
    "            quality_ratings = highlight_review.find_all(\"li\", {\"class\": \"recommend-answer\"})\n",
    "            if quality_ratings != None:  \n",
    "                for rating in quality_ratings:\n",
    "                    description = rating.find(\"div\", {\"class\": \"recommend-description\"})\n",
    "                    description = description.find(text = True)\n",
    "                    score = rating.find(\"span\")            \n",
    "                    score = score['alt']\n",
    "                    if   description == \"Value\":    value = score[0]\n",
    "                    elif description == \"Location\": locat = score[0]\n",
    "                    elif description == \"Sleep Quality\": sleep = score[0]\n",
    "                    elif description == \"Rooms\": rooms = score[0]\n",
    "                    elif description == \"Cleanliness\": clean = score[0]\n",
    "                    elif description == \"Service\": servi = score[0]\n",
    "                    else: other = score[0]\n",
    "                    \n",
    "            page_review = url['id'] + sep + title + sep + body + sep + rati + sep \\\n",
    "                        + value + sep + locat + sep + sleep + sep + rooms + sep + clean + sep + servi + sep + other\n",
    "            page_review_string = page_review_string + page_review + \"\\n\"\n",
    "    return(page_review_string)\n",
    "\n",
    "\n",
    "def hotel_run(base_urls, sub_urls, citys, cutNo_hotels, cutNo_reviews):\n",
    "    # This is the starting function for the web scraping of hotel reviews fro tripAdvisor website.\n",
    "    # Writes the review deatils of each hotel into hotelName-review.mcsv file in the city folder\n",
    "    \n",
    "    timeStart = datetime.strptime(datetime.now().strftime(dtfmt), dtfmt)  # gets the starting time\n",
    "    print(\"\\n******* Starting time[\" + str(timeStart) + \"] Scraping reviews for \" + str(cutNo_hotels) + \" hotels ******************\")    \n",
    "    if os.path.exists(DataDir + citys): shutil.rmtree(DataDir + citys) # checks if directory exists and delete it\n",
    "    os.makedirs(DataDir + citys) # make directory\n",
    "    hotel_Urls(base_urls, sub_urls, citys) # call function\n",
    "    \n",
    "    #read file into dataframe\n",
    "    hotel_df = pd.read_csv(DataDir + citys + \"/\" + citys + \"_hotel_file.mcsv\", sep=\"~\", header=None, names = ['id', 'name', 'urlh', 'urlt', 'rat', 'reno'])\n",
    "    hotel_df.drop_duplicates(['id'], inplace=True) # drops duplicate reviews \n",
    "    hotel_df.reset_index(inplace=True) # Reset the index of the dataframe\n",
    "    \n",
    "    for index, row in hotel_df.iterrows():\n",
    "        timeIndex = datetime.strptime(datetime.now().strftime(dtfmt), dtfmt) \n",
    "        output_string = \"\"\n",
    "        hotel_name = row['name']\n",
    "        head_urls = row['urlh']\n",
    "        tail_urls = row['urlt']\n",
    "        no_review = int(row['reno'].replace(',','')) \n",
    "        if no_review > cutNo_reviews: no_review = cutNo_reviews\n",
    "        _or_value = 0\n",
    "        \n",
    "        if index > cutNo_hotels: # cutoff number of hotels to study\n",
    "            timeEnd = datetime.strptime(datetime.now().strftime(dtfmt), dtfmt)\n",
    "            print(\"*********** Finished with used time[\" + str((timeStart - timeEnd)/60) + \" mins] ********************************\")\n",
    "            break \n",
    "            \n",
    "        print(\"\\tStarting time[\" + str(timeIndex) + \"] \" + str(index) + \": Scraping \" + str(no_review) + \" reviews of \" + hotel_name) \n",
    "        while (_or_value < no_review):\n",
    "            if _or_value == 0: _or = \"\"\n",
    "            else:  _or = \"or\" + str(_or_value) + \"-\"\n",
    "            reviewpg_url = base_urls + head_urls + _or + tail_urls \n",
    "            output_string = output_string + hotel_review(reviewpg_url)\n",
    "            _or_value = _or_value + 10   \n",
    "        review_file = open(DataDir + citys + \"/\" + hotel_name + \"-review.mcsv\", \"a+\", encoding=\"utf-8\") \n",
    "        review_file.write(output_string)\n",
    "        review_file.close()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Starting time[2017-05-04 12:37:42] Scraping reviews for 40 hotels ******************\n",
      "\tStarting time[2017-05-04 12:37:50] 0: Scraping 50 reviews of Little_National_Hotel\n",
      "\tStarting time[2017-05-04 12:39:31] 1: Scraping 50 reviews of Aria_Hotel_Canberra\n",
      "\tStarting time[2017-05-04 12:41:16] 2: Scraping 50 reviews of East_Hotel\n",
      "\tStarting time[2017-05-04 12:42:41] 3: Scraping 50 reviews of Forrest_Hotel_And_Apartments\n"
     ]
    }
   ],
   "source": [
    "cutNo_hotel = 40 # Number of hotels to scrap from each city\n",
    "cutNo_review = 50 # Number of reviews to scrap from each hotel\n",
    "\n",
    "# Url of the city review list divided into base and sub urls\n",
    "base_url = \"http://www.tripadvisor.com.au\"\n",
    "sub_url = \"/Hotels-g255057-Canberra_Australian_Capital_Territory-Hotels.html\"\n",
    "city = \"Canberra\" # Name of a city\n",
    "#sub_url = \"/Hotels-g255060-Sydney_New_South_Wales-Hotels.html\",  city = \"Sydney\"\n",
    "#sub_url = \"/Hotels-g255100-Melbourne_Victoria-Hotels.html\", city = \"Melbourne\"\n",
    "#sub_url = \"/Hotels-g255068-Brisbane_Brisbane_Region_Queensland-Hotels.html\", city = \"Brisbane\"\n",
    "hotel_run(base_url, sub_url, city, cutNo_hotel, cutNo_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
