{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hotel Review Sentiment Analysis Part 2: EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline \n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import bigrams, trigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter\n",
    "#nltk.download() # download the english stopwords corpus and the punkt package and maybe the porter stemmer if not present\n",
    "\n",
    "pd.set_option('display.max_columns', 36)\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review_feats = ['id', 'title','body','rati','value','locat','sleep','rooms','clean','servi','other']\n",
    "citys = ['Canberra', 'Sydney', 'Melbourne', 'Brisbane']\n",
    "review_df = pd.DataFrame() #creates a new dataframe that's empty\n",
    "for city in citys:\n",
    "    citydir = os.path.join(os.getcwd(), city)\n",
    "    for file in glob.glob(os.path.join(citydir,\"*-review.mcsv\")): \n",
    "        review_df = review_df.append(pd.read_csv(os.path.join(citydir, file), sep=\"~\", header=None, names = review_feats), ignore_index=True)\n",
    "\n",
    "print(review_df.shape)\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_dfx = review_df[['id', 'title', 'body', 'rati']].copy()\n",
    "review_dfx['rati'] = pd.to_numeric(review_dfx['rati'], errors='coerce') \n",
    "#print(review_dfx.rati.unique()) , print(review_dfx.shape)\n",
    "review_dfx = review_dfx.dropna(axis=0)\n",
    "print(review_dfx.rati.unique())\n",
    "print(review_dfx.shape)\n",
    "review_dfx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rat_cat = 5\n",
    "colors = np.array(['#E50029', '#E94E04', '#EEC708', '#A5F30D', '#62F610']) # 1, 2, 3, 4, and 5 ratings respectively\n",
    "\n",
    "rat_labels = np.array([x_rat+1 for x_rat in range(rat_cat)])\n",
    "rat_cat_dist_fig = plt.figure(figsize=(12,8))\n",
    "bar_plot_indices = np.arange(rat_cat)\n",
    "rat_cat_absolute_freq = review_dfx.rati.value_counts(ascending=True)\n",
    "rat_cat_relative_freq = np.array(rat_cat_absolute_freq)/float(sum(rat_cat_absolute_freq))\n",
    "\n",
    "rects = plt.bar(bar_plot_indices, rat_cat_relative_freq, width=1, color=colors, alpha=.7)\n",
    "for (idx, rect) in enumerate(rects):\n",
    "        plt.gca().text(rect.get_x()+rect.get_width()/2., 1.05*rect.get_height(), \n",
    "                '%d'%int(rat_cat_absolute_freq[idx+1]), ha='center', va='bottom')\n",
    "\n",
    "plt.xticks(bar_plot_indices+.5, rat_labels)\n",
    "plt.xlabel('Rating Category')\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.ylim([0,1])\n",
    "plt.title('Rating Category Distribution for {0} Reviews'.format(len(review_dfx)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Some words (e.g. no, not, more, most etc.) have been removed from the standard stopwords available in NLTK. \n",
    "#It’s done so because those words can have some sentiment impact in our review dataset.\n",
    "cloud_stopwords = set(stopwords.words('english')) | set(('hotel', 'room', 'apartment', 'bathroom', 'bedroom', 'bed', 'also'))\n",
    "\n",
    "def preproc(dseq):\n",
    "    data_list = []\n",
    "    for s in dseq:\n",
    "        for ch in string.punctuation:  s = s.replace(ch, \"\") \n",
    "        for dg in string.digits:  s = s.replace(dg, \"\") \n",
    "        s = s.lower()\n",
    "        data_list.append(s)\n",
    "    return data_list\n",
    "\n",
    "def wordcloud_draw(data, color = 'black'):\n",
    "    words = ' '.join(data)\n",
    "    cleaned_word = \" \".join([word for word in words.split()])\n",
    "    wordcloud = WordCloud(stopwords=cloud_stopwords, background_color=color, width=2500, height=2000).generate(cleaned_word)\n",
    "    plt.figure(1,figsize=(12, 12))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating = 5\n",
      " ;907754\n",
      "e;506383\n",
      "t;374829\n",
      "a;369136\n",
      "o;319984\n",
      "n;266210\n",
      "i;259218\n",
      "r;257205\n",
      "s;235537\n",
      "l;193928\n",
      "h;183702\n",
      "d;161061\n",
      "c;120601\n",
      "u;108376\n",
      "f;106911\n",
      "w;103950\n",
      "y;100911\n",
      "m;93052\n",
      "g;78722\n",
      "p;69361\n",
      "b;67770\n",
      "v;51671\n",
      "k;37102\n",
      "x;10809\n",
      "q;6427\n",
      "j;4921\n",
      "z;3225\n",
      "’;174\n",
      "é;126\n",
      "–;61\n",
      "…;27\n",
      "”;17\n",
      "“;16\n",
      "️;14\n",
      "⭐;10\n",
      "，;10\n",
      "´;8\n",
      "‘;7\n",
      "☺;6\n",
      "£;6\n",
      "​;5\n",
      "▪;4\n",
      "❤;4\n",
      "ç;3\n",
      "要;2\n",
      "ô;2\n",
      "你;2\n",
      "—;2\n",
      "‎;2\n",
      "à;2\n"
     ]
    }
   ],
   "source": [
    "def prepdata(dseq):\n",
    "    worddata = \"\"\n",
    "    for s in dseq:\n",
    "        for ch in string.punctuation:  s = s.replace(ch, \"\") \n",
    "        for dg in string.digits:  s = s.replace(dg, \"\") \n",
    "        s = s.lower()\n",
    "        worddata = worddata + \" \" + s\n",
    "    return worddata \n",
    "\n",
    "review_dfy = review_dfx[review_dfx['rati'] == 5]\n",
    "print(\"Rating = 5\")\n",
    "\n",
    "#words = ' '.join(preproc(review_dfy['body']))\n",
    "cleaned_word = \" \".join([word for word in prepdata(review_dfy['body']).split()])         \n",
    "\n",
    "# Calculate frequency distribution\n",
    "fdist = nltk.FreqDist(cleaned_word)\n",
    "# Output top 50 words\n",
    "for word, frequency in fdist.most_common(50):\n",
    "    print(u'{};{}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms_bigram[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_dfy = review_dfx[review_dfx['rati'] == 5]\n",
    "print(\"Rating = 5\")\n",
    "wordcloud_draw(preproc(review_dfy['body']),'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_dfy = review_dfx[review_dfx['rati'] == 3]\n",
    "print(\"Rating = 3\")\n",
    "wordcloud_draw(preproc(review_dfy['body']),'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_dfy = review_dfx[review_dfx['rati'] == 1]\n",
    "print(\"Rating = 1\")\n",
    "wordcloud_draw(preproc(review_dfy['body']),'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, roc_auc_score\n",
    "from sklearn import cross_validation, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review_dfz = review_dfx[['rati', 'body']]\n",
    "for idx in range(5):\n",
    "    print(review_dfz.body[idx])\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++\\n\")    \n",
    "print(\"\\n**************************************************************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Some words (e.g. no, not, more, most etc.) have been removed from the standard stopwords available in NLTK. \n",
    "#It’s done so because those words can have some sentiment impact in our review dataset.\n",
    "customised_stopwords = set(stopwords.words('english')) - set(('over', 'under', 'below', 'more', 'most', 'no', 'not', 'only',\n",
    "                                                            'such', 'few', 'so', 'too', 'very', 'just', 'any', 'once'))\n",
    "def remove_stopwords(s):\n",
    "    token_list = nltk.word_tokenize(s)\n",
    "    exclude_stopwords = lambda token : token not in customised_stopwords\n",
    "    return ' '.join(filter(exclude_stopwords, token_list))\n",
    "\n",
    "def stem_token_list(token_list):\n",
    "    STEMMER = PorterStemmer()\n",
    "    return [STEMMER.stem(tok) for tok in token_list]\n",
    "\n",
    "def restring_tokens(token_list):\n",
    "    return ' '.join(token_list)\n",
    "\n",
    "def preprocess(s):\n",
    "    for ch in string.punctuation:  s = s.replace(ch, \"\") \n",
    "    for dg in string.digits:  s = s.replace(dg, \"\") \n",
    "    s = s.lower()\n",
    "    \n",
    "    s = remove_stopwords(s)\n",
    "    token_list = nltk.word_tokenize(s)\n",
    "    token_list = stem_token_list(token_list)\n",
    "    return restring_tokens(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_dfz['body'] = review_dfz['body'].apply(preprocess)\n",
    "\n",
    "for idx in range(5):\n",
    "    print(review_dfz.body[idx])\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "print(\"\\n**************************************************************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(review_dfz.shape)\n",
    "review_dfz.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(review_dfz.body, review_dfz.rati, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_of_words_vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                          tokenizer = None,    \n",
    "                                          preprocessor = None,\n",
    "                                          ngram_range = (1, 1),\n",
    "                                          binary = False,\n",
    "                                          strip_accents='unicode')\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                    tokenizer = None,\n",
    "                                    preprocessor = None,\n",
    "                                    ngram_range = (2, 2),\n",
    "                                    strip_accents='unicode')\n",
    "\n",
    "trigram_vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                     tokenizer = None,\n",
    "                                     preprocessor = None,\n",
    "                                     ngram_range = (3, 3),\n",
    "                                     strip_accents='unicode')\n",
    "\n",
    "bi_and_trigram_vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                            tokenizer = None,\n",
    "                                            preprocessor = None,\n",
    "                                            ngram_range = (2,3),\n",
    "                                            strip_accents='unicode')\n",
    "\n",
    "random_forest_vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                           tokenizer = None,\n",
    "                                           preprocessor = None,\n",
    "                                           ngram_range = (1,1),\n",
    "                                           strip_accents = 'unicode',\n",
    "                                           max_features = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(bag_of_words_vectorizer.vocabulary_), print(\"++++++++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "def get_range(d, begin, end):\n",
    "    result = {}\n",
    "    for (key,value) in d.iteritems():\n",
    "        if key >= begin and key <= end:\n",
    "            result[key] = value\n",
    "    print(result)\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "    \n",
    "get_range(bigram_vectorizer.vocabulary_, 0, 9)\n",
    "\n",
    "#print(trigram_vectorizer.vocabulary_), print(\"++++++++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "\n",
    "#print(bi_and_trigram_vectorizer.vocabulary_), print(\"++++++++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "\n",
    "#print(random_forest_vectorizer.vocabulary_), print(\"++++++++++++++++++++++++++++++++++++++++++++++++\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_confusion_matrix_relative(confusion_matrix):\n",
    "    star_category_classes = [1, 2, 3, 4, 5]\n",
    "    N = list(map(lambda clazz : sum(Y_test == clazz), star_category_classes))\n",
    "    relative_confusion_matrix = np.empty((len(star_category_classes), len(star_category_classes)))\n",
    "    \n",
    "    for j in range(0, len(star_category_classes)):\n",
    "        if N[j] > 0:\n",
    "            relative_frequency = confusion_matrix[j, :] / float(N[j])\n",
    "            relative_confusion_matrix[j, :] = relative_frequency\n",
    "            \n",
    "    return relative_confusion_matrix\n",
    "\n",
    "# http://www.wenda.io/questions/4330313/heatmap-with-text-in-each-cell-with-matplotlibs-pyplot.html\n",
    "# http://stackoverflow.com/questions/20520246/create-heatmap-using-pandas-timeseries\n",
    "# http://sebastianraschka.com/Articles/heatmaps_in_r.html\n",
    "# http://code.activestate.com/recipes/578175-hierarchical-clustering-heatmap-python/\n",
    "def plot_confusion_matrix(confusion_matrix=[[]], title='CM', savefilename=''):\n",
    "    rcm = make_confusion_matrix_relative(confusion_matrix)\n",
    "    #plt.imshow(rcm, vmin=0, vmax=1, interpolation='nearest')\n",
    "    c = plt.pcolor(rcm, edgecolors='k', linewidths=4, cmap='jet', vmin=0.0, vmax=1.0)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(0.5 + np.arange(5), np.arange(1,6))\n",
    "    plt.yticks(0.5 + np.arange(5), np.arange(1,6))\n",
    "\n",
    "    def show_values(pc, fmt=\"%.2f\", **kw):\n",
    "        #from itertools import zip\n",
    "        pc.update_scalarmappable()\n",
    "        ax = pc.get_axes()\n",
    "        for p, color, value in zip(pc.get_paths(), pc.get_facecolors(), pc.get_array()):\n",
    "            x, y = p.vertices[:-2, :].mean(0)\n",
    "            if sum(color[:2] > 0.3) >= 2:\n",
    "                color = (0.0, 0.0, 0.0)\n",
    "            else:\n",
    "                color = (1.0, 1.0, 1.0)\n",
    "            ax.text(x, y, fmt % value, ha=\"center\", va=\"center\", color=color, **kw)\n",
    "    \n",
    "    show_values(c)\n",
    "\n",
    "    if savefilename:\n",
    "        plt.savefig(savefilename, bbox_inches='tight')\n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "def print_classifier_performance_metrics(name, predictions):\n",
    "    target_names = ['1 star', '2 star', '3 star', '4 star', '5 star']\n",
    "    \n",
    "    print (\"MODEL: %s\" % name)\n",
    "    print ()\n",
    "\n",
    "    print ('Precision: ' + str(metrics.precision_score(Y_test, predictions, average='micro')))\n",
    "    print ('Recall: ' + str(metrics.recall_score(Y_test, predictions, average='micro')))\n",
    "    print ('F1: ' + str(metrics.f1_score(Y_test, predictions,  average='micro')))\n",
    "    print ('Accuracy: ' + str(metrics.accuracy_score(Y_test, predictions)))\n",
    "\n",
    "    print()\n",
    "    print ('Classification Report:')\n",
    "    print (classification_report(Y_test, predictions, target_names=target_names))\n",
    "    \n",
    "    print()\n",
    "    print ('Precision variance: %f' % np.var(precision_score(Y_test, predictions, average=None), ddof=len(target_names)-1))\n",
    "    \n",
    "    print()\n",
    "    print ('Recall variance: %f' % np.var(recall_score(Y_test, predictions, average=None), ddof=len(target_names)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_feature_matrix_train = bag_of_words_vectorizer.fit_transform(X_train)\n",
    "bow_feature_matrix_test = bag_of_words_vectorizer.transform(X_test)\n",
    "bow_feature_matrix_train, bow_feature_matrix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multinomial_nb_classifier = MultinomialNB()\n",
    "multinomial_nb_classifier.fit(bow_feature_matrix_train, Y_train)\n",
    "multinomial_nb_prediction = multinomial_nb_classifier.predict(bow_feature_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multinomial_confusion_matrix = confusion_matrix(Y_test, multinomial_nb_prediction)\n",
    "print (make_confusion_matrix_relative(multinomial_confusion_matrix))\n",
    "plot_confusion_matrix(multinomial_confusion_matrix, 'Multinomial Naive Bayes Confusion Matrix', savefilename='MultinomialCM.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_classifier_performance_metrics('Multinomial Naive Bayes', multinomial_nb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_multinomial_feature_matrix_train = bigram_vectorizer.fit_transform(X_train)\n",
    "bigram_multinomial_feature_matrix_test = bigram_vectorizer.transform(X_test)\n",
    "#bigram_multinomial_feature_matrix_train, bigram_multinomial_feature_matrix_test\n",
    "\n",
    "#Make predictions with Trigram Multinomial NB\n",
    "bigram_multinomial_nb_classifier = MultinomialNB().fit(bigram_multinomial_feature_matrix_train, Y_train)\n",
    "bigram_multinomial_nb_prediction = bigram_multinomial_nb_classifier.predict(bigram_multinomial_feature_matrix_test)\n",
    "\n",
    "#Visualize through confusion matrix\n",
    "bigram_multinomial_confusion_matrix = confusion_matrix(Y_test, bigram_multinomial_nb_prediction)\n",
    "plot_confusion_matrix(bigram_multinomial_confusion_matrix, 'Bigram Multinomial Naive Bayes Confusion Matrix', savefilename='BigramMultinomialCM.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_classifier_performance_metrics('Bigram Multinomial Naive Bayes', bigram_multinomial_nb_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram Naive Bayes Model\n",
    "\n",
    "## Transform Yelp reviews into feature vectorizers by counting bigram occurrences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_multinomial_feature_matrix_train = trigram_vectorizer.fit_transform(X_train)\n",
    "trigram_multinomial_feature_matrix_test = trigram_vectorizer.transform(X_test)\n",
    "#trigram_multinomial_feature_matrix_train, trigram_multinomial_feature_matrix_test\n",
    "\n",
    "#Make predictions with Trigram Multinomial NB\n",
    "tri_gram_multinomial_nb_classifier = MultinomialNB().fit(trigram_multinomial_feature_matrix_train, Y_train)\n",
    "tri_gram_multinomial_nb_prediction = tri_gram_multinomial_nb_classifier.predict(trigram_multinomial_feature_matrix_test)\n",
    "\n",
    "#Visualize through confusion matrix\n",
    "trigram_multinomial_confusion_matrix = confusion_matrix(Y_test, tri_gram_multinomial_nb_prediction)\n",
    "plot_confusion_matrix(trigram_multinomial_confusion_matrix, 'Trigram Multinomial Naive Bayes Confusion Matrix', savefilename='TrigramMultinomialCM.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_classifier_performance_metrics('Trigram Multinomial Naive Bayes', tri_gram_multinomial_nb_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest 100 Learners Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest100 = RandomForestClassifier(n_estimators = 100, random_state=42)\n",
    "\n",
    "#Transform Yelp reviews into feature vectors\n",
    "random_forest_feature_matrix_train = random_forest_vectorizer.fit_transform(X_train)\n",
    "random_forest_feature_matrix_test = random_forest_vectorizer.transform(X_test)\n",
    "\n",
    "#Make predictions with random forest set at 100 learners\n",
    "%time forest100.fit(random_forest_feature_matrix_train.toarray(), Y_train)\n",
    "forest100_pred = forest100.predict(random_forest_feature_matrix_test.toarray())\n",
    "np.save('forest100pred', forest100_pred)\n",
    "\n",
    "#Visualize results in confusion matrix\n",
    "random_forest_confusion_matrix = confusion_matrix(Y_test, forest100_pred)\n",
    "plot_confusion_matrix(random_forest_confusion_matrix, 'Random Forest (100 Learners) Confusion Matrix', savefilename='RandomForestCM.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_classifier_performance_metrics('Random Forest (100 Learners)', forest100_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
