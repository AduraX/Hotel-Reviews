{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hotel Review Sentiment Analysis Part 2: EDA\n",
    "## Adura ABIONA, PhD (UNSW)\n",
    "### 4 May, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This is the **Part 2** of the **Hotel Review Sentiment Analysis** of Australian hotels, from four major cities (Canberra, Sydney, Melbourne and Brisbane), based on reviewers' opionions (on a numerical scale of 1-5) from [**TripAdvisor**](http://www.tripadvisor.com.au) website. This part is focused on **Exploratory Data Analysis (EDA)** of the reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'WordCloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-20f299a18779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mENGLISH_STOP_WORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'WordCloud'"
     ]
    }
   ],
   "source": [
    "import glob, os, string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline \n",
    "sep = \"~\"\n",
    "DataDir = \"Datasets/\" \n",
    "#nltk.download() # download the english stopwords corpus and the punkt package and maybe the porter stemmer if not present\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whereis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The block of code below reads the review details for the hotels from the 4 major cities in Australia into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review_feats = ['id', 'title','body','rati','value','locat','sleep','rooms','clean','servi','other']\n",
    "citys = ['Canberra', 'Sydney', 'Melbourne', 'Brisbane']\n",
    "review_df = pd.DataFrame() #creates a new dataframe that's empty\n",
    "for city in citys:\n",
    "    citydir = os.path.join(os.getcwd(), DataDir + city)\n",
    "    for file in glob.glob(os.path.join(citydir,\"*-review.mcsv\")): \n",
    "        review_df = review_df.append(pd.read_csv(os.path.join(citydir, file), sep=sep, header=None, names = review_feats), ignore_index=True)\n",
    "\n",
    "print(review_df.shape)\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_dfx = review_df[['id', 'title', 'body', 'rati']].copy()\n",
    "review_dfx['rati'] = pd.to_numeric(review_dfx['rati'], errors='coerce') \n",
    "review_dfx = review_dfx.dropna(axis=0)\n",
    "print(review_dfx.rati.unique())\n",
    "print(review_dfx.shape)\n",
    "review_dfx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating Category Distribution for Reviews\n",
    "The rating categories (1,2,3,4, and 5) are from now on color coded with red, orange, blue, purple and green respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rat_cat = 5\n",
    "colors = np.array(['#E50029', '#E94E04', '#EEC708', '#A5F30D', '#62F610']) # 1, 2, 3, 4, and 5 ratings respectively\n",
    "\n",
    "rat_labels = np.array([x_rat+1 for x_rat in range(rat_cat)])\n",
    "rat_cat_dist_fig = plt.figure(figsize=(12,8))\n",
    "bar_plot_indices = np.arange(rat_cat)\n",
    "rat_cat_absolute_freq = review_dfx.rati.value_counts(ascending=True)\n",
    "rat_cat_relative_freq = np.array(rat_cat_absolute_freq)/float(sum(rat_cat_absolute_freq))\n",
    "\n",
    "rects = plt.bar(bar_plot_indices, rat_cat_relative_freq, width=1, color=colors, alpha=.7)\n",
    "for (idx, rect) in enumerate(rects):\n",
    "        plt.gca().text(rect.get_x()+rect.get_width()/2., 1.05*rect.get_height(), '%d'%int(rat_cat_absolute_freq[idx+1]), ha='center', va='bottom')\n",
    "\n",
    "plt.xticks(bar_plot_indices+.5, rat_labels)\n",
    "plt.xlabel('Rating Category')\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.ylim([0,1])\n",
    "plt.title('Rating Category Distribution for {0} Reviews'.format(len(review_dfx)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Some words (e.g. no, not, more, most etc.) have been removed from the standard stopwords available in NLTK. \n",
    "#It’s done so because those words can have some sentiment impact in our review dataset.\n",
    "custom_stopwords = set(stopwords.words('english') + [\"n't\", \"'ve\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS) \n",
    "                 + ['hotel', 'room', 'apartment', 'bathroom', 'bedroom', 'bed', 'also'] \n",
    "                 + ['canberra', 'sydney', 'melbourne', 'brisbane']) - set(('over', 'under', 'below', 'more', 'most', 'no', \n",
    "                                            'not', 'only', 'such', 'few', 'so', 'too', 'very', 'just', 'any', 'once'))\n",
    "PuncSym = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"--\", \"-\", \"...\", \"..\", \"“\", \"”\"]\n",
    "\n",
    "def preprocess(sentx):\n",
    "    for ch in string.punctuation:  sentx = sentx.replace(ch, \" \") \n",
    "    for dg in string.digits:  sentx = sentx.replace(dg, \" \") \n",
    "    sentx = sentx.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    sentx = sentx.lower()    \n",
    "    wordList = [word for word in sentx.split() if word not in custom_stopwords] # Given a list of words, remove any that are in a list of stop words.\n",
    "    sentx = ' '.join(wordList)\n",
    "    token_list = nltk.word_tokenize(sentx) \n",
    "    #STEMMER = PorterStemmer()\n",
    "    #token_list = [STEMMER.stem(tok) for tok in token_list]\n",
    "    return token_list\n",
    "\n",
    "def wordcloud_draw(data, color = 'black'):\n",
    "    words = ' '.join(data)\n",
    "    cleaned_word = \" \".join([word for word in words.split()])\n",
    "    wordcloud = WordCloud(stopwords=custom_stopwords, background_color=color, width=2500, height=2000).generate(cleaned_word)\n",
    "    plt.figure(1,figsize=(12, 12))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a list of words, return a dictionary of word-frequency pairs.\n",
    "def wordListToFreqDict(wordlist):\n",
    "    wordfreq = [wordlist.count(p) for p in wordlist]\n",
    "    freqdict = dict(zip(wordlist, wordfreq))\n",
    "    aux = [( freqdict[key], key) for key in freqdict]\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_dfy5 = review_dfx[review_dfx['rati'] == 5]\n",
    "print(\"Rating = 5\")\n",
    "joinStr5 = preprocess(' '.join(review_dfy5['body'].tolist()[:100]))\n",
    "sorteddict5 = wordListToFreqDict(joinStr5)\n",
    "itr5 = 0\n",
    "for s in sorteddict5: \n",
    "    print(str(s))\n",
    "    if itr5 > 4: break \n",
    "    itr5 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Rating = 5\")    \n",
    "wordcloud_draw(joinStr5,'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_dfy3 = review_dfx[review_dfx['rati'] == 3]\n",
    "print(\"Rating = 3\")\n",
    "joinStr3 = preprocess(' '.join(review_dfy3['body'].tolist()[:100]))\n",
    "sorteddict3 = wordListToFreqDict(joinStr3)\n",
    "\n",
    "itr3 = 0\n",
    "for s in sorteddict3: \n",
    "    print(str(s))\n",
    "    if itr3 > 4: break \n",
    "    itr3 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Rating = 3\")\n",
    "wordcloud_draw(joinStr3,'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_dfy1 = review_dfx[review_dfx['rati'] == 1]\n",
    "print(\"Rating = 1\")\n",
    "joinStr1 = preprocess(' '.join(review_dfy1['body'].tolist()[:100]))\n",
    "sorteddict1 = wordListToFreqDict(joinStr1)\n",
    "itr1 = 0\n",
    "for s in sorteddict1: \n",
    "    print(str(s))\n",
    "    if itr1 > 4: break \n",
    "    itr1 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Rating = 1\")\n",
    "wordcloud_draw(joinStr1,'white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Still in progress ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud of 1 to 5 star reviews\n",
    "We will interface with both Python and R in order to make the wordclouds. \n",
    "Python to format the ngram count data, and R to visualize the data respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os # os.chdir()\n",
    "MY_YELP_OUTPUT_DIR = os.path.join(os.getcwd(), 'Output_Directory')\n",
    "REVIEW_STARS_BIGRAMS_DIRECTORY = os.path.join(MY_YELP_OUTPUT_DIR, 'review_stars_bigrams')\n",
    "REVIEW_STARS_TRIGRAMS_DIRECTORY = os.path.join(MY_YELP_OUTPUT_DIR, 'review_stars_trigrams')\n",
    "\n",
    "if not os.path.exists(MY_YELP_OUTPUT_DIR): os.makedirs(MY_YELP_OUTPUT_DIR)\n",
    "if not os.path.exists(REVIEW_STARS_BIGRAMS_DIRECTORY): os.makedirs(REVIEW_STARS_BIGRAMS_DIRECTORY)\n",
    "if not os.path.exists(REVIEW_STARS_TRIGRAMS_DIRECTORY): os.makedirs(REVIEW_STARS_TRIGRAMS_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for xstars in range(0, 1):\n",
    "    bigrams = [\"%s %s\" % bi for bi in nltk.bigrams(stars_vs_cat_texts[xstars+1])]\n",
    "    bigrams_df = DataFrame.from_dict(Counter(bigrams).most_common(len(stars_vs_cat_texts[xstars+1])))\n",
    "    bigram_csv_filepath = os.path.join(REVIEW_STARS_BIGRAMS_DIRECTORY, '{0}_star.csv'.format(xstars+1))\n",
    "    bigrams_df.to_csv(bigram_csv_filepath, index=False)\n",
    "    \n",
    "    trigrams = [\"%s %s %s\" % tri for tri in nltk.trigrams(stars_vs_cat_texts[xstars+1])]\n",
    "    trigrams_df = DataFrame.from_dict(Counter(trigrams).most_common(len(stars_vs_cat_texts[xstars+1])))\n",
    "    trigram_csv_filepath = os.path.join(REVIEW_STARS_TRIGRAMS_DIRECTORY, '{0}_star.csv'.format(xstars+1))\n",
    "    trigrams_df.to_csv(trigram_csv_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import rpy2\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(tm)\n",
    "library(wordcloud)\n",
    "library(RColorBrewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -i MY_YELP_OUTPUT_DIR,REVIEW_STARS_TRIGRAMS_DIRECTORY,REVIEW_STARS_BIGRAMS_DIRECTORY\n",
    "\n",
    "palettes <- c(\"Reds\", \"Oranges\", \"Blues\", \"Purples\", \"Greens\")\n",
    "\n",
    "for (xstars in 1:5) {\n",
    "    xstars.bigrams.path <- paste(REVIEW_STARS_BIGRAMS_DIRECTORY, '/', xstars, '_star.csv',sep='')\n",
    "    xstars.trigrams.path <- paste(REVIEW_STARS_TRIGRAMS_DIRECTORY, '/', xstars, '_star.csv',sep='')\n",
    "    xstars.bigrams.df <- read.csv(xstars.bigrams.path)\n",
    "    xstars.trigrams.df <- read.csv(xstars.trigrams.path)\n",
    "    xstars.wordcloud.df <- rbind(xstars.bigrams.df[1:100,], xstars.trigrams.df[1:100,])\n",
    "    xstars.wordcloud.df <- xstars.wordcloud.df[with(xstars.wordcloud.df, order(-X1, X0)),]\n",
    "    \n",
    "    pal <- brewer.pal(9, palettes[xstars])\n",
    "    pal <- pal[-(1:3)]\n",
    "    png(paste(MY_YELP_OUTPUT_DIR, '/', xstars, '_star_wordcloud.png',sep=''), width=960, height=960)\n",
    "    wordcloud(xstars.wordcloud.df$X0, xstars.wordcloud.df$X1 , max.words=200, colors=pal)\n",
    "    dev.off()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import Image\n",
    "Image(filename=os.path.join(MY_YELP_OUTPUT_DIR, '1_star_wordcloud.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
